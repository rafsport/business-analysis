{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    text = [p.text for p in doc.paragraphs]\n",
    "    return text\n",
    "\n",
    "file_path = r\"C:\\Users\\Raffaele.Sportiello\\OneDrive - Wolters Kluwer\\Documents\\Presentazioni\\TAAE Data & Analytics\\TAAE Data & Analytics Show & Tell NA.docx\"\n",
    "doc_text = read_docx(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Trascrizione',\n",
       " '24 luglio 2023, 03:04PM',\n",
       " '\\nJeroen si è unito alla riunione',\n",
       " '\\nDonny   0:09\\nGo.\\nAll right.\\nI am confident everybody can see this right?',\n",
       " '\\nImma   0:25\\nYes.',\n",
       " \"\\nDonny   0:25\\nAlright, fantastic.\\nSo, yes, hello everyone.\\nI'm Donny gore.\\nI'm part of the TAA operations team, part of Amise team.\\nDoing well, we'll talk more about it, but BI data science, data analytics.\\nUh, But the road map planning, etcetera.\\nSo we do, we do quite a few different things.\\nSo let's let's jump in and we'll, we'll talk through what agenda to start with.\\nSo yeah, what?\\nWe're gonna go through.\\nI think Alisdair laid it out pretty well in the email, but just to recap, the roles and responsibilities of the team, what kind of tools and technology that we're leveraging today along with some upcoming plans for some of that technology, some of the primary data sources that we use, I think some of you guys will be familiar with some, you won't, but we'll kind of talk through those.\\nThe current architecture of the global data warehouse, along with ETL tools.\\nSome of the different data sources again kind of talk through how we get some of that, UM and then some of the things that we ran into would around data privacy and security, both you know, initiatives that we've done internally to the team, but as well as I'm sure as all of you are aware, things that come down from GBS or or TAA division on what we can and can't do with data or things that we should keep track of.\\nSo we'll talk about some of those as well.\\nAnd of course, any point, feel free to stop me, ask a question.\\nClarify whatever the case may be.\\nSo of feel free to be interactive.\\nSo the first thing that will start with is again, kind of the rules and responsible as the team.\\nWhen Garrett joined, I think he had all of his direct reports put these together.\\nSo let me put what he has gathered.\\nIt's really nice way to kind of understand and break down and and share what the team is.\\nSo it's an organizational functional decomposition which is just really fun to say fast as well.\\nIf you guys want to try it so the mission that we had to define for the team was to accelerate and evolve AI driven technology and data solutions that provide our customers and agents globally with exceptional digital experiences.\\nSo and and how we try to attempt to do that is like I said we we break up the team into to multiple functionality, multiple functions a lot of times if you hear me talk about this.\\nSo talk about pieces of a pie, which I think is a pretty good analogy to help explain that so.\\nAI solution development.\\nThis is 1 where we largely Ava, right, is probably the one that yell of the heart of the most.\\nThis is something that we partnered with DXG for the development side of things, but we own like product ownership, we help drive the Sprint planning Rd maps prioritization, do analysis on technology of how we should solve things, how we shouldn't.\\nSo definitely stray is a lot more out of the oh, I do data analyst work right.\\nWe kind of own that whole thing and then with the data background, we're able to provide them data to open up additional functionality, additional features, et cetera.\\nSo it's a really good synchronization that we found with that team business intelligence, business intelligence and I think this is the one that we probably talk about most about at least in these groups.\\nAnd so this is exactly what you think.\\nIt's our data warehouse.\\nIt's how we store the data, how we process the data.\\nWe also do business analysis work.\\nWe don't have any dedicated BAS.\\nIt's part of what we do with our team is lots of different hats.\\nSo a lot of times the same person that will be writing the SQL query will have worked with the business to understand exactly what metric you're trying to pull out.\\nYou trying to understand?\\nHelp them understand the data, etcetera.\\nSo we try to work really closely with the business and almost embed folks in.\\nSo we understand their process is why we're doing these things, et cetera, right?\\nAnd then agile project management on top of that, we use JIRA and we used two week sprints.\\nWe go through every other week, plan out what are we gonna do the next two weeks?\\nOur business users can create stories out there a lot of times will work with them to create stories.\\nA lot of leadership likes to email.\\nI am right, things like that.\\nSo we try to be somewhat flexible but still keep that process around.\\nSo we can we can manage it.\\nIt's a far cry from back in the day when you just manage everything over email and hope for the best, right?\\nSo we can actually track have a backlog.\\nAll those things, it's very nice.\\nAnd then finally, our data science and machine learning.\\nThis is, you know, applied statistics.\\nLargely, uh.\\nSo things like forecasting call volume for the North America centers is something that we do umm, we do have folks that are writing π and or to to or Python And R to do different things.\\nData Discovery data analysis.\\nYou know all the applied statistics, things that you would think about.\\nSo as we go through it, these three that we're gonna largely touch on, I think this will resonate most with what this group is and kind of what we're talking about.\\nI will on occasion though, make reference to some of the other things that are part of the teams per view, right customer support sites.\\nSo in North America, we've got more support sites and you could possibly imagine for whatever reason.\\nSo we capture data off of that, obviously, but then also a part of a mess team actually maintains develops, right, all those things with the support portals and then they technology enablements relatively new in the last year, 18 months or so.\\nAnd this is where we set down in a lot.\\nI think a lot of you on this call probably help participate this in one way or another where we define like that technology roadmap.\\nWe talked about what we were gonna do from an operations group globally, right?\\nWe started to lay those things out and define those things and really start to get alignment.\\nSo I think that's gonna be a huge portion of what we do.\\nYou know, outside of these other functional areas as well.\\nUmm, so I think it'll be really good just from a TAA standpoint to to get aligned more on those.\\nSo any questions on this slide, I think you're expecting to hear that didn't, OK?\\nThe antastic.\\nLet's go and let's talk about tools and technology.\\nSo the first one, kind of the bedrock of of what we do from a data standpoint, we use Amazon Redshift in our data warehouse.\\nIt's posted in AWS for those of you familiar with Postgres SQL.\\nAmazon Redshift is based on that, so syntax functions, et cetera largely go from one to the other, from Postgres to redshift.\\nThere are some exceptions to that that are well documented, though Amazon doesn't really good job with their documentation.\\nUmm, similar to some other BI data warehouse utilities, you probably heard about or storage.\\nIt's columnar storage, right?\\nSo it's designed to pull back billions of rows of or millions of columns of data back to do analytics on top of it.\\nAnd it's multi massive parallel processing, right?\\nSo queries get split off across multiple nodes to run, that cetera and then one of the nice things about AWS.\\nUmm, just the integration with their ecosystem, right, with the amount of bring your own code things with Lambda functions?\\nUmm, all the things you can do with US3, storage and querying etcetera.\\nIt just plays really nice with some of the ecosystem that we've already.\\nKind of invested in built in, for example Ava, that we talked about before.\\nIt's all built in AWS, so there's again a lot of overlap that we can do in terms of storage, like where Ava writes its data, doesn't write directly to S3.\\nAs an example, we write it out to a Dynamodb that then gets detailed over right.\\nSo not having to worry about infrastructure crossing over through, you know Azure or AWS etcetera just makes life a little bit easier.\\nSo we've been pretty happy with redshift learning curve, but it's performed pretty well indeed.\\nAnd then for kind of the presentation layer or a data visualization as Tableau now we went with Tableau originally there was a lot of synergy within WK, our Legal Solutions Group was already using it internally.\\nSo we were able to kind of buddy system along with them onto that server and then and it was a great tool, right, very innovative, easy to learn, easy to use, very intuitive, just just made the active charts, graphs, understanding data so much easier.\",\n",
       " '\\nRaffaele ha lasciato la riunione',\n",
       " \"\\nDonny   9:01\\nAnd then as probably a lot of you know Salesforce Bot tableau not terribly long ago and I think since that's happened, we've seen change is not that we consider for the better right in terms of that innovative features kind of fallen off, they don't release this new new new features as they they used to and we kind of see it slowly evolving into a tool specific to Salesforce, right.\",\n",
       " '\\nRaffaele si è unito alla riunione',\n",
       " \"\\nDonny   9:27\\nIt really just their internal reporting tool.\\nSo we're gonna be starting the migration process to power BI.\\nThere's a lot of moving parts to that, but there are some just some benefits that you we couldn't turn down.\\nCost this one right, everybody.\\nCentrally, WK has a free Office 365 Power BI license which is kind of the equivalent to a viewer in Tableau.\\nSo there's a big economic function there, and from a feature functionality standpoint, not a lot of differences, right?\\nTableau does some things better.\\nPower BI does some things better, but we think it'll be pretty successful with Power BI and again just kind of speak to that synergy within.\\nWK and TAA specifically.\\nSo we're going to be a lot, but we're excited about it as well.\\nAnd then finally, the kind of the third pillar of our tools and technology is talented.\\nIt's what we use for our ETL data integration processes.\\nUmm.\\nBut he was not familiar.\\nTalent is is there's an open source version which has some pluses and minuses, right?\\nOpen source cost is always good.\\nSometimes support for those open source products are less so.\\nOhm talent also pretty open in terms of custom code, custom processes that talent can then incorporate, so makes it pretty handy if you've got more technical data engineers on that side.\\nThat being said, it has been a while since we've looked at our ETL tools, so we are exploring options.\",\n",
       " '\\nRaffaele si è unito alla riunione',\n",
       " \"\\nDonny   10:55\\nWe're looking at AWS glue.\\nWe looked at that.\\nNot terribly long ago.\\nUmm, there are some pieces missing that we would need from a talent perspective, but we're hoping that that will continue to build out again.\\nSeems like a pretty natural choice.\\nAnd then the other thing with acquisitions, talent has recently been bought by click.\\nSo some of the same concerns that we've had that we've seen with Tableau, I think we have with talent as well.\\nDoes this term into a click exclusive tool right?\\nThose types of things.\\nSo something that we're keeping an eye on, but right now everything works pretty well and you know change in infrastructure is often required, but not something you necessarily wanna do lightly.\\nSo we're gonna take those all things slowly and and properly so, alright.\\nSo let's talk about data sources now.\\nA lot of these you'll be familiar with a lot of data sources that we deal with.\\nSo I think some of the ones that you're probably familiar with Salesforce, I think everybody is dealing with that now.\\nWe've got multiple instances of Salesforce which is still a little odd to me, but that's where we're at.\\nSo service cloud is where our support teams right and within operations log cases create knowledge, those types of things.\\nSo very important again, I think pretty standard for Europe at this point as well.\\nOur sales and marketing team and some of our OEM stuff, sorry, order management stuff happens in sales cloud, which is a fully separate instance.\\nNow there are some.\\nThere are some projects they're supposed to happen this summer that will change how some of that works.\\nThere's a was an org connector.\\nI think it is where you can connect two separate Salesforce instances and then I think there is still a project next year to migrate and combine Salesforce instances, so that'll be a lot.\",\n",
       " '\\nRaffaele si è unito alla riunione',\n",
       " \"\\nDonny   12:39\\nSo to work through, that's the kind of the back of a lot of the reporting that we do.\\nAnd then Active Directory we use pretty heavily for permissions, obviously, but then reporting hierarchies, so a lot of our a lot of reports are agent based.\\nAs an example, and so it's I wanna see my team see show the people that report to me.\\nSo we let Active Directory kind of define all that stuff and we we do pull some dimensions over for it, but it's largely for that, that person reporting that we do and it less admin overhead for us, right.\\nWe don't have to maintain those relationships.\\nSAP is another big one.\\nIf it's not big in your world of Europe yet, it will be before long.\\nThis is the, of course, the ERP system.\\nWe get a lot of customer dimensions from this inventory, etcetera about those customers and then Genesis, this is what the US and Canada uses for their telephony system.\\nSo you'll often heard this referred to as I3.\\nThat's the old name of the company.\\nIt's just kind of stuck around, but support calls, chats, uh, web tickets, right?\\nUh, all of those things route through Genesys.\\nAnd so, historically and traditionally, this is how we've measured work done by agents, right?\\nThis is all the key metrics came out of this interactions handled your your handle time, your talk time, your wrap up time right, all those very traditional call center KPIs that you would expect these are I I think Genesys will become less important for that over time as we kind of move to this digital first aspect right and strategy and that will have a lot less of those one to one inbound type calls happening.\\nBut that's the hope for the transition anyway.\\nThe other fun thing about Genesis also next year, there are plans to migrate from our on Prem solution to a cloud based solution.\\nSo not only will we migrating our CRM system, we will on the heels of that be migrating our telephony system, so should be a really fun time with data migration and data quality checks and all of that kind of stuff.\\nSo but it will be good to finally get on.\\nThere's a lot of technology limitations we have with the on premise, so there's gonna be a lot of work to get there, but it will definitely be worth it once we are.\\nAnd then we have some data sources that I pray none of you ever have to deal with at all.\\nLotus notes is still something that's a thing in in North America.\\nAs hard as we have tried to kill it, it cannot be done apparently.\\nSo we get a lot of data from like our tax development teams that do like the forms development.\\nThey'll document data in here.\\nAlso, some of the processes that are tax development teams have for like escalations, asking for help right?\\nThings like that rolling through lodestones as well.\\nSo there's fingers crossed there's a plan to migrate this as well regulate these workflows out of Lotus notes into JIRA that it would be fantastic.\\nBut again, I've been here 18 years and for about 18 years I've been hearing that Lotus notes is going away, so we'll see.\\nBut again, just another data source that we have to deal with and then some that are.\\nUnique somewhat to North America, at least things that we've built in internally.\\nSo in House applications, our survey application today is in house built.\\nSo for that post contact survey, so we use that for a lot of information, CSAT transactional NPS.\\nWe're gonna change a little bit to meet some new KPIs that we're talking about from a division, but we're maybe not quite there yet and we'll maybe be transitioning that out to a different solution as well.\\nBut today, that's a main data source.\\nAnd again, that's something that we build and and house, uh, the customer portals, we've kind of touched on a little bit for the US that they're well for North America.\\nI guess there's four today, so four different support portals that we support as a team, but then also able to gather data out of it.\\nAnd then of course, Ava, Umm, shout out to Netherlands here for this incredible graphic that they created for us.\\nThis is my favorite thing, but Ava, I think you all know about for us North America, we get full data out of Ava.\\nWhat happened in the session?\\nWho did what?\\nWho was the user?\\nAll those types of things we do reporting on European Ava data, but we just get pre aggregated.\\nSo we just know basically days and totals, we don't know who we don't know what just because of privacy concerns, et cetera, hope to change that in the future.\\nBut so these are some of the main data sources that we we use day in and day out and the the business uses for decision making etcetera.\\nSo no questions on that.\\nI will move on and we'll talk about the architectural, but I'm not gonna go into great detail here, but I'll.\\nI'll touch on these.\\nSo the first part over here, these are some of those data sources that we just kind of talked about in this.\\nIn this instance, they're split up.\\nBasically, is source type right?\\nSo we've got ones that are true databases, right?\\nThere's a SQL back end.\\nThere's some type of back end that we can query directly to get that data.\\nSources is API, so service cloud and sales cloud both have a fully featured API where you can pull those that data back.\\nSo from those systems we do that and then we have others here where we get basically exports from other teams, right, Twilio.\\nIt's one we haven't talked about, but Twilio is.\\nIt's a voiceover IP solution, so you could build self service apps, telephony system, front end type things, voice menus and tool.\\nWe use that for uh, we've got a voice bot that we're still kind of piloting, but we get logs from them, the support site.\\nSo we've talked about SAP.\\nSAP was one that we don't query directly for various reasons, but if that they are able to generate export files.\\nSo we deal with those back then through talent and S3 and then here are some of the Ava data and you can see this one a little bit more involved in some of those others.\\nWe're able.\\nWe're pushing the Dynamo DB.\\nThere's some Landis in there to help do redaction for PII data, and then the Kinesis data streams will push that out.\\nAnd then here's why I talk about like we've got the Lambda functions, so data source wise and then over here, right, we've actually got our EC2 machines that actually kind of drive all these jobs S3 where we preload state all the data we have another process.\\nSo in additional redaction process that we do for things like phone records, right where we learn, what the, you know, your call your phone number was, when we go through Salesforce and hash out email addresses, right, things of that nature.\\nAnd then we finally store all that data back in redshift, and then of course, we have either live connections from Tableau into Redshift or Tableau has a data sources that you can build or data extracts that you can build.\\nBut we, from a visualization reporting dashboard standpoint, the what the kind of the expectations that we set is if it lives in a database, if it lives in a place that we can get to it via API via whatever, we can absolutely lead that need to warehouse.\\nIf your answer is I have a spreadsheet that I can email you every week, we're like, that's great.\\nFigure out how to get a database and we will help you with that.\\nSo we try very hard from a standards and structure standpoint to not do these one off manual processes because we just know they're going to fall apart.\\nSo we try to stick to.\\nIf we can't automate the loading of the data, capturing the data, et cetera, it's not something we can report on right now.\\nUmm what one offs happened on occasion?\\nSure.\\nBut nothing that's gonna stand like six months from now.\\nThere's not gonna be a manual process behind these things.\\nWe just, we just can't sustain it so.\\nAnd then you can see processes going back and forth to today.\\nWe've got some processes here.\\nLike AI for web case, it'll be going through some changes, but what this AI process does it actually leverages the data in the data warehouse to make decisions on.\\nIn this case, customer asked a question.\\nIs there a Salesforce article that would help for it?\\nWhile this user's waiting for an agent to pick up, the AI process runs strictly up the data warehouse data, and then we'll push data back into service cloud.\\nTo let an agent or a customer know an answer.\\nSo we do use, we take data from the data warehouse and do push in a production systems.\\nThe other place we do that is an Ava will look at the logs and basically churn through a new team and what are popular topics based on where a user's coming from, and we'll use that data to.\\nPrepopulate and enhance and augment what Ava is doing from a an insight perspective.\\nSo we have and we try to 7 the team with lots of lots of scope allows us to kind of filter in things from a data perspective in different places where maybe normally we wouldn't have just with our involvement.\\nSo any question on diagram?\\nI know a relatively quick, but I think it's pretty straightforward for the audience here.\\nOK? Yeah.\",\n",
       " '\\nAlisdair   21:51\\nI think the interesting to explore that point.\\nNot not now, but later was of what you guys are doing with that article recommendation.',\n",
       " '\\nDonny   22:00\\nMm-hmm.',\n",
       " '\\nAlisdair   22:00\\nUmm.\\nAnd pushing it back into sales force and what the difference is between evil and because obviously caved in North America and are are the teams doing it side by side or?',\n",
       " '\\nDonny   22:08\\nYeah.',\n",
       " \"\\nAimee   22:10\\nWe don't use so umm.\",\n",
       " '\\nDonny   22:13\\nYeah.',\n",
       " \"\\nAimee   22:15\\nSo I happen to own coveo too.\\nSo coveo in North America, there's a suggestions on the side.\\nWe don't use their last chance deflection, frankly, coveo's outrageously expensive.\\nLet me just be honest.\\nOhh and so we don't use their recommendation cause we've found through side by side testing that they're recommendation engines that we have from our AI Ava and AI for web case suggestion provides superior answers because there's more.\\nOh, I see myself.\\nThat's weird.\\nThere's more recommendations.\\nOhh, and there's more interactions there in training Alisdair.\\nSo while we do use like you know when you're typing in a web case, you get that side panel that like it's changing and it's giving you recommendations.\\nNorth America uses that we uh, there's also a feature that coveo calls their last chance to flexion.\\nSo when you are done typing up your ticket and you hit the submit button, I wanna submit my ticket, then coveo takes you through a process of three screens that provide a recommendation to you.\\nWe evaluated that and found that tool to not be as part of that tech roadmap.\\nThat Donnie discussed, we found that tool to not be over the user friendly and not to be able to provide answers with the same level level of accuracy that are other two AI engines could.\\nSo in North America, what we've what we're releasing, that's the enhancement that Donny just talked about is when you hit the submit button for the ticket, it immediately just like an Ava gives you what we call our last chance deflection and then you can say, yes, that answers my question or no submitted ticket.\\nSo it's a one additional screen for a customer instead of three.\\nI hope that answered your question Alisdair if it didn't happen to expand.\",\n",
       " '\\nAlisdair   24:03\\nNo, it does, but it opens up more questions which will leave for just now, but will maybe pick them up separately.',\n",
       " '\\nAimee   24:09\\nYeah.',\n",
       " '\\nAlisdair   24:11\\nUh, wait.',\n",
       " \"\\nAimee   24:11\\nYeah.\\nAnd just so you're aware, we're discussing coveo with Cecile and Ernest and Coveo's great in certain areas.\\nThere's other areas that.\\nUmm.\\nThrough our tech road map reviews, we have found other solutions, both internal and external, that can handle certain things that coveo wants to handle maybe better than they do.\\nThat's kind of where we're at today.\",\n",
       " \"\\nAlisdair   24:34\\nYeah.\\nYeah, I think that's the conversation that kind of want to have with you.\",\n",
       " '\\nAimee   24:39\\nSure.\\nHappy to do that.',\n",
       " \"\\nAlisdair   24:40\\nSo, so yeah.\\nUnderstand what it needs, cause we're not just thinking about using coveo within the support context.\\nWe're also thinking about using kathio within the product, but if you think of better ways to handle that, it would be good to explore those.\",\n",
       " \"\\nAimee   24:52\\nUmm.\\nAbsolutely.\\nAnd you guys are at a different, I will say like for the UK, coveo provides a much better service because there's multiple sources of knowledge.\\nSo you like you've got your user docs and you've got your knowledge base, so convey is really good about that.\\nIf you think about a North America, we just have or knowledge based articles, so you're not getting the advantage that coveo is providing to like Elaine in the UK team because it's providing them one search engine across multiple systems.\",\n",
       " '\\nAlisdair   25:23\\nYeah.',\n",
       " \"\\nAimee   25:24\\nWe're in North America.\\nIt's one search engine for one system, so we're not quite getting the bang for our buck that you guys are.\",\n",
       " '\\nAlisdair   25:31\\nYeah.\\nOK.\\nGood to know though, how expected this time side by side OK.',\n",
       " '\\nAimee   25:34\\nBut yeah, happy to talk.',\n",
       " \"\\nDonny   25:35\\nYeah.\\nYeah.\\nNo, there's lots of lots of discussions on coveo and I'm sure they'll they'll be more so.\\nBut yeah, definitely one we should.\\nWe should have more.\\nAlright, thank you Alisdair.\\nUh, let's move on.\\nLet's talk about some of the future stuff and and some of the places that we're already today and where we want to get better.\\nSo just kinda try to chunk these back up into four kind of main kind of topics and concepts that will hopefully resonate because I think it's things we all struggle with.\\nSo the first one is data alignment.\\nThis is something I think we're and I'll talk about some steps we're taking to do this as well, but I think through the we did an initiative for the SVE balance scorecard.\\nI guess not.\\nThe operations balance scorecard, where the goal was really let's let's all get together as TAA operations, right?\\nAll the support teams, all the everything, and let's let's define what our metrics are.\\nLet's define what our KPIs are.\\nLet's make definitions on what those things are, which seems like kind of a no brainer.\\nBut it's not something that we ever done, and unsurprisingly, there were differences of opinions between Europe and the US and other places on what something should mean and what was important.\\nSo it was difficult conversations, but I think we got to a place where we all kind of agreed and I think that's super important.\\nOne of the biggest challenges I think we had is or we we we have our people accidentally misrepresenting metrics and and what things mean you know common things like wait time, what is wait time mean to you, do we have a standard definition somewhere that says what it is, right what those calculations are.\\nSo that's something that we're working on from a cataloging, cataloging, defining standpoint.\\nBut then also just getting consensus within groups on what what do these things mean and what do they mean to you data quality is is another area I I think that's a pretty constant refrain I think for many but that works with data a lot of times.\\nIt starts at the source system level.\\nWe've we've got instances where as an example in service cloud folks there, there are teams that will use a field one way and then we've got folks that use a field a different way, different types of data, different total meetings.\\nAnd that really is, is you all know it really hard.\\nIt's hard to make generic reporting off of things when everything is unique to a team or a group, and even things within, even within the US, right there are teams there that just that lack of consistency just cause us so many data issues.\\nSo again, I think wrapping more process around that more documentation governance et cetera will help drive some of those things and have a clear start to finish line where we can track that data to know what it is, et cetera.\\nSo a couple of things that we're doing to to improve this, right?\\nIt's never gonna be perfect, cause humans are always gonna be involved, but we're collaborating with the CXP or the customer experience program.\\nSo Chris Manno and Mike Mundis join Garrett's team relatively recently.\\nRecently, and so they're really driving things more from a master data management.\\nWhat's the process look like?\\nWhat data element should we capture?\\nWho stewards that data, et cetera.\\nSo we'll be collaborating with them quite a bit.\\nThey've got a tool called the name just escaped me, the Libra, a Libra.\",\n",
       " '\\nAimee   28:56\\nThe Libra.',\n",
       " \"\\nDonny   28:59\\nUh, that will be partnering with them with and we're already doing a one of the first things I think that they're working with is actually connecting to our data warehouse to start looking at data legacy and those types of things.\\nSo we can start working with that.\\nSo really excited to work on that side of it.\\nI don't know about you.\\nAll the documentation always seems to be the last thing that gets done, and there's also the thing that gets pushed off because while we've got work to actually do too, so it'll be nice to have a process and a program in place kind of forces some of that documentation.\\nUh, that's just gonna make us better.\\nAnd then two of the other areas workflow I mentioned it before we we transitioned into JIRA and Agile to try to do those things.\\nI think there's still improvements that can absolutely be had.\\nDon't think any workflow is perfect.\\nSo as we know, we're gonna bring Canada into the fold more.\\nSo that's something that we've got to work with and just making sure that we've got the right folks in those workflows.\\nWe've got the right check-ins and meetings in place to take temperatures of users and groups to make sure they're getting what they need and just leaving that door open for communication.\\nSo.\\nSo everybody's clear.\\nUmm it?\\nLike I said, talking about before, it's data is really easy to misrepresent.\\nReally easy to misinterpret, so I think it's important to have those check-ins with the businesses and the the folks that are using that data just to make sure that we're clear.\\nThe last thing that we want to do is get called into a meeting where they say look at this data.\\nWhat is it?\\nLook this way and we have to go.\\nOh, that's not right.\\nThat doesn't look correct, so working with that, bringing that data integrity and data literacy up for the business and that'll it's it's somewhat selfish.\\nIt frees our team up to do more exciting work than just building reports and walking users through how to read them, but at the same time, it just ensures that nobody gets surprised or upset, because that's always the worst thing to happen with data causes scrambling, we lose lack of trust, right?\\nAll those things.\\nAnd then finally, I didn't have a fun caption to put on this one, but collaboration right?\\nWorking together, I think not only from a data perspective, but I think I feel like we're all kind of going through some transition.\\nWe're all, you know, we've had work change in North America.\\nWe're working with groups we've never worked with before, so they can make that collaboration is gonna be key to make sure that we're actually solving issues from a global level, which is again something new.\\nI think for a lot of folks and some of the areas that I think they're gonna help us drive this, that I've seen right creation of that tech roadmap forcing everyone to get in a room together and talk about these are the strategies.\\nThese are the things that we want to work on and not what technology helps enable that and drive that.\\nUmm, now in a lot of cases we got out of the room in two weeks later, somebody said, oh, I want to look at this vendor.\\nI wanna do this new thing that I didn't have any idea about two weeks ago.\\nThat's always gonna happen, and that's certainly encouraged, right.\\nInnovation and trying new things is great, but also having a plan that we check it in quarterly and do we need to update it, are we doing the right things is Nice as well.\\nSo VSP, I've talked about that a little bit on some of the work that we do with that.\\nI think Amie takes a much larger role in that, but maybe we did in the past.\\nSo I think that coordination of VSP is gonna be super important, especially when we start talking about like what Europe's doing, what US is doing like are we are we trying separate things to get to the same goal.\\nSo I think that again, that alignment and uh, learning best practices from teams etcetera I think will help.\\nWhat we do a lot and and just make things better and then that digital first strategy right where we are purposely saying like we self service digital first all those things are initiative and imperative.\\nBut again, how do we support it?\\nHow do we measure it?\\nHow do we produce the data that that makes sense about it et cetera?\\nSo any questions on any of these?\\nAll right, cool. Uh.\\nAnd then finally, data privacy and security.\\nAnd apparently it's lawn mowing day, so apologize if you guys can hear that.\\nSo the first area that we really that in the US at least in tax and accounting, that security really became a thing for us that hey, the government saying to do this, IRS 7216 was updated around 2010.\\nThe basically said this is what you can and can't do with taxpayer data.\\nIt governed thing and this really hit hard on the development and support side because I don't know, I mean you guys remember, I mean there were days where just like, yeah, just give me that return and we'll troubleshoot it.\\nAnd now I've got somebody's return that I probably shouldn't have to work on and troubleshoot.\\nAnd then there were instances of outsourcing, right?\\nSending taxpayer data outside the US without their consent?\\nA lot of things like that.\\nSo this really opened up the door of, oh, we've got a scrub data.\\nWe can't have this privacy data around the next big one that I came.\\nI'm assuming you've all heard of GDPR at this point.\\nThat's a joke.\\nI know you all have and it's giving you headaches.\\nUh, so I think you'll know about that one, but that's caused a massive shift.\\nI think everybody was here on security and privacy and then there's a new one for North America called Quebec Law 25.\\nThat's kind of in the process of being implemented.\\nI really like the way it was described at.\\nThe aim is to overhaul the privacy regime in Quebec, which just sounds, I don't know, like a little dystopian, but still a good, good effort.\\nWe're not quite sure what this one's gonna do today, that we know it's gonna have an impact, but how it's gonna be done, all those things a little up in the air.\\nSo I think I've got a lot of Michaels this year to learn more about it, to make sure that we're good.\\nBut then really, the answer is partnering with legal teams that you know when you're reading my email aloud in a deposition.\\nI didn't do anything to break the law as the goal.\\nSo and then the other, we do some process, I touched on it before, but getting PII data out of the data warehouse was a huge effort that we undertook, and it's an ongoing process that we constantly look at.\\nSo this is looking at case records to make sure that we get people's names redacted, we get email addresses hashed because we still need to know that user record.\\nBut we don't want to display who that user is on reporting things like that.\\nSo we go through an additional process.\\nWe rely on what the source system does to redact data for things like Ava, things like that, right?\\nWe pull data out of that, but we do another pass to try to pull those out and then we've got some tools that help us do that.\\nA AWS itself from a security standpoint, it's really around locations of servers, right?\\nThis is one of the first things just, you know, people govern where their data can live without consent.\\nSo we have to keep that in mind and then also just again some of the bring your own code stuff right for finding patterns, those types of things, Amazon account comprehend is what we're using for our speech to text part of it AWS solution but comprehend will allow us to redact that sensitive information out of transcripts from phone calls before we try to summarize or provide information about that.\\nAnd then these last two are really more around Salesforce operations.\\nI know AWS IDP.\\nUh, it it's a W solution.\\nIt's not unique to Salesforce, but we're using it.\\nFor is when people attach PDFs or excel files or whatever, just to make sure that we're not providing that, we're not letting that pass through.\\nThat's more on the support portal side of things, but I have a feeling it will be a process that we end up using other places as well for our Web ticket management or other things, right where there's documents that we have to make sure safe before we attach those systems.\\nFrom a PII perspective?\\nAnd then finally, Salesforce Shield is something that's being implemented.\\nI think it's still this year that's being implemented, right?\\nAnd that's supposed to help with also with uh PII data within Salesforce.\\nInterested to see haven't seen exactly what this is gonna do to our data from a Salesforce perspective and what kind of impact that will have on etls, data integration, et cetera.\\nSo something we're gonna have to vent and look at, but it will be a nice piece of mind knowing that there's gonna be less PII data coming through our data transformation jobs that we have to worry about and be concerned on.\\nSo that is it for me.\\nSo happy to answer any questions or there's anything I didn't cover that you expected happy to talk about those.\",\n",
       " \"\\nAlisdair   37:25\\nThen probably be been chasing to hear again me.\\nBut more than depth of about, I thought I effectively AWS filter that you've got there for filtering and out PID to just submit it.\\nUmm same so.\",\n",
       " '\\nDonny   37:40\\nYeah.',\n",
       " '\\nAimee   37:41\\nWhich one?',\n",
       " \"\\nAlisdair   37:49\\nYeah, just to eat both.\\nWell, for what?\\nWhat?\\nUnderstandably, but more because it feels like there's use cases that you guys had explored in in a bit further ahead than we are.\",\n",
       " '\\nAimee   37:55\\nNo. So.\\nSure.',\n",
       " \"\\nAlisdair   38:02\\nWe're not implementing those kind of technologies.\\nUh on our side, but maybe we should be thinking a little bit more global than less and and find out what you guys have achieved.\\nYou know, just you explain it and and and take us through it in more depth.\",\n",
       " '\\nAimee   38:13\\nYeah.',\n",
       " \"\\nAlisdair   38:16\\nAnd again, I don't necessarily think it's for data and analytics.\\nI think it's the right tools in place to achieve a specific job that helps data and analytics, but it feels that's like more more on almost like your other remit as a as a unit.\",\n",
       " '\\nDonny   38:26\\nYeah.',\n",
       " '\\nAimee   38:28\\nSure.\\nYeah.',\n",
       " '\\nDonny   38:30\\nYeah.',\n",
       " \"\\nAimee   38:31\\nSo let me give you.\\nYeah.\\nLet me give you a high level.\\nSo I IDP, so you're gonna get tired of hearing me say I own that.\\nSo, so IDPIDP is Amazon intelligent document processing.\\nSo how that particular tool works from Amazon is it has the ability to scan a PDF, a Word document, an Excel file, a video, pretty much almost any attachment.\\nAnd within that attachment it can identify what that is.\\nSo when a customer attaches something to, let's say a web ticket and it's a tax return, IDP can tell you that is a 1040 form.\\nIt's a big tax form in the US it can tell you what that form is.\\nThen there's a secondary part of IDP.\\nIf it is a document, not a video or a but a document, it can go in and actually redact.\\nSo if it knows it's a 1040 form, it knows on a 1040 form that a customer's Social Security number, maybe they're address their name, it knows where that is on the form and it can go in and redact that.\\nSo we are in the process right now.\\nI actually just got an email a few minutes ago.\\nIf signing the SOW with Amazon to have their professional services come in and build the work, build the solution for us.\\nIf everybody hasn't heard, I'm a huge fan of Amazon and part of why I'm a huge fan fan of Amazon is they are the most.\\nCollaborative vendor that I have ever had the opportunity to work with.\\nYou'll hear me talk.\\nI'll tell you about comprehend and just a second Alisdair, but on the comprehend project that we're working on, Donny and I were in a meeting with them.\\nWe've been in multiple meetings as a matter of fact, where we've said, you know, it'd be really nice if it could do this, but that's not a must have.\\nAnd they literally show up to the next meeting.\\nAnd they're like, hey, I got bored at dinner last night and I coated that nice to have, and here it is for free.\\nNo questions asked.\\nSo we're big fans of Amazon, so we're bringing them in to do that as part of what's the tax law.\\nI can never remember, Donny, that you literally just had on the screen that I 7216.\",\n",
       " '\\nDonny   40:43\\n72167216 yeah, sure.',\n",
       " \"\\nAimee   40:45\\nThank you.\\nI should have that number memorized, but I've got, like, a mental blocker against it.\\nSo we're doing that as part of that project Alisdair I'm Salesforce Shield is going into the production later this.\\nI've already moved to August.\\nLater in August, they've already started development of that, and we're supposed to have IDP in place by the end of August too.\\nThen comprehend is part of our speech to text project.\\nSo in North America, 70% of our interactions with customers happen over a phone, whether that be an inbound call or an outbound call.\\nAnd so we're transcribing all of those calls from speech to text, and we don't want personally identifiable information to exist at all anywhere in that speech to text.\\nI don't even want to hold it temporarily in flight, so Amazon comprehend literally redacts it as they're transcribed.\\nSolution is transcribing it so the minute I say you know my Social Security number is, it automatically starts redacting it and then it's smart enough to put in brackets to let us know what kind of data it encrypted, because I would assume maybe people in Europe are better than Americans.\\nBut in America, we hand out our Social Security numbers and our credit cards like they're like Flyers to like a concert.\\nIt's kind of crazy.\\nAnd then we get mad when somebody takes them.\\nSo it's one of the things that we're working to try to help support customers and prevent happening.\\nI hope that gave you a high level Alisdair of what those are and how we're using them from a AI BI support site perspective.\",\n",
       " '\\nAlisdair   42:14\\nOK.\\nYeah.\\nYeah.\\nSo we we can wait a bit similar tools with Microsoft as the IDP tool to mass data both in image and video ohm.',\n",
       " '\\nAimee   42:29\\nMm-hmm.\\nMm-hmm.',\n",
       " \"\\nAlisdair   42:35\\nIt said so.\\nIf it's just a proof of concept that we did this part of code games, but the same idea was there and that we would build that kind of capability actually into the digital portal so that when we're capturing data they are and we're submitting it through screen capture or video capture, it's going away and it's getting redacted.\",\n",
       " '\\nAimee   42:56\\nMm-hmm.\\nMm-hmm.',\n",
       " \"\\nAlisdair   43:01\\nIf if you've got more advanced solution in the US or further down the line and would prefer that we actually aligned on tool sets there again, so not necessarily part of the data analytics piece, but almost part of the funnel.\",\n",
       " '\\nAimee   43:13\\nYeah.',\n",
       " '\\nAlisdair   43:14\\nIt feels almost part of the instrumentation part of what you guys do.',\n",
       " '\\nAimee   43:19\\nIt is.',\n",
       " \"\\nAlisdair   43:19\\nI think I think there's a lot more we could share and use from what you've taken us through today.\\nOhm and we could we could make the doctor now.\",\n",
       " \"\\nAimee   43:27\\nYeah.\\nAnd we looked at.\\nYeah.\\nAnd just so you're aware, we looked at the Microsoft one, Amazon had a few additional features that were important to the US that we needed and the cost was astronomically different.\\nThe Amazon one is like $0.07 per document or something like that.\\nSo substantially lower than what Microsoft had quoted, and that seven cents before our WK, discounting to then it gets down to like pennies on the dollar, which is awesome.\",\n",
       " '\\nAlisdair   43:57\\nYou could.\\nOK.\\nInteresting.\\nMaybe definitely need to pick that up and have a conversation about that.',\n",
       " '\\nAimee   44:04\\nUmm.',\n",
       " \"\\nAlisdair   44:06\\nOK, I'm the one thing I was gonna ask was about data sources.\",\n",
       " '\\nAimee   44:07\\nHappy too.\\nSorry, Donny.',\n",
       " '\\nDonny   44:13\\nUmm.',\n",
       " '\\nAlisdair   44:14\\nYou know, I wondered if you were consuming any data from product directly, you know, so obviously quite an ecosystem of or supporting enterprise applications.',\n",
       " \"\\nDonny   44:23\\nYeah.\\nWe're not, sadly.\\nWe want to. Good.\",\n",
       " '\\nAimee   44:29\\nWith the with the one POC that amaze doing.',\n",
       " '\\nDonny   44:33\\nYeah.',\n",
       " \"\\nAimee   44:33\\nBut that's manual, so that's why it's POC.\",\n",
       " \"\\nDonny   44:33\\nFor yeah, yeah, the, the the project is a churn prediction model for the engagement product, right.\\nOne of the audit audit products, so it's hit or miss on what products are collecting things on Prem is a probably never gonna happen and unfortunately got a huge on Prem customer base with pfx.\",\n",
       " '\\nAlisdair   44:53\\nYou.',\n",
       " \"\\nDonny   44:56\\nBut for the cloud based and why aren't we?\\nIt makes it makes sense.\\nAnd then it's, you know, it's getting PM Dash, you're getting product to share that data and be open to sharing that data and all those things come up.\\nBut yeah, absolutely.\\nWe'd love to some type of prediction and and or at least do a study on user.\\nUser did this in the software and then didn't user logged off support call.\\nHow frequently does that happen?\\nRight where we can leverage those things and get a more free 60 view.\\nSo definitely want it.\",\n",
       " '\\nAimee   45:23\\nAnd we have some hooks in Ava that are waiting for that data Alisdair to make Ava better engagement.',\n",
       " '\\nDonny   45:27\\nYeah.',\n",
       " \"\\nAimee   45:30\\nThe POC that MA, Donny and my partners doing.\\nUh is using pendo, but to Donny's point and part of why it's not on any of our graphics is that it's a file that sent over to us today.\\nSo they've been told very strictly that that is a POC until they can get that in a more automated fashion.\",\n",
       " '\\nDonny   45:49\\nYeah.',\n",
       " '\\nAlisdair   45:50\\nOK.\\nSo on our side, you has the county.\\nEurope has, uh, just agreed the Pendle will be the divisional tool of choice.',\n",
       " '\\nDonny   46:06\\nGood.',\n",
       " \"\\nAlisdair   46:07\\nSo, uh, we had initially gone with amplitude and due to some GDPR thoughts, but that's gonna.\\nI were gonna refer out that decision.\\nSo we'll see Pendo rolled out across the European landscape for anybody that does want to use one of those kind of tools.\\nSo hopefully we can evolve that prefer concept.\\nI know that obviously Ben and Ed, that demonstrates that they were consuming some pendo data already.\\nSo yeah, there's overlap there that we can possibly look at.\\nOK, good. Interesting.\",\n",
       " '\\nJeroen   46:44\\nAnother question, how about the connection you have to the Active Directory?',\n",
       " '\\nDonny   46:49\\nUh-huh.',\n",
       " '\\nJeroen   46:50\\nBecause I thought that was quite a smart way of making.\\nAlso, the user systems data driven.\\nSo how did you set it up?\\nOr what connection do you have there?',\n",
       " \"\\nDonny   47:01\\nI believe we've just we're we're occurring LDAP directly.\\nUh, it's it's been a long time since we've done it.\\nI can get technical details for you though on on how we're getting that data because we're getting it.\\nWe're getting it from the NA domain, but I believe we can plug into the EU domain as well, so it should be theoretical, but I'll get more specifics for you, Darren, and and let you see that.\",\n",
       " '\\nAimee   47:23\\nYeah, we have access to NASA, APAC and EU and the only reason I know that Donny is cause of May was having to pull it for Kiran the other day.',\n",
       " '\\nDonny   47:28\\nYeah.\\nOh, OK.',\n",
       " '\\nAimee   47:32\\nSo so I know we can get to all of them.',\n",
       " \"\\nDonny   47:35\\nBut I, but I think there's a a relatively standard way to query L DAP and get that information and we've we've gone through some trials and tribulations with it just because of formatting and other things.\\nSo yeah, I'll get the information and I'll just provide it this whole group.\",\n",
       " '\\nJeroen   47:46\\nUmm.',\n",
       " '\\nDonny   47:48\\nSo you bug got it, but yeah.',\n",
       " '\\nJeroen   47:50\\nYeah, that would be would be useful.',\n",
       " '\\nDonny   47:52\\nSure.',\n",
       " '\\nJeroen   47:52\\nIt makes sense to use that as a means of keeping your filters correct.',\n",
       " \"\\nDonny   47:55\\nWhen it's yeah, when it's when it's maintained well, we're where we're running into some trouble is things like contractors.\\nSometimes they wanna put all the contractors under one person just to for back office reasons and that now all those people report in that person, so they show up on the report.\",\n",
       " '\\nJeroen   48:08\\nUmm.',\n",
       " '\\nDonny   48:13\\nSo sometimes you get to deal with those things, but for the most part it drives so much in the organization that they will keep that up to date, and that will always be accurate.',\n",
       " '\\nJeroen   48:22\\nYeah.\\nYeah, makes sense.',\n",
       " '\\nDonny   48:24\\nYeah.',\n",
       " \"\\nJeroen   48:24\\nAnd then the other question and because I I think the Netherlands currently is one of the heaviest users of Ava and I do see some of the reports coming by from from Richard, where he's he's showing the US that if boards but then whenever he has questions I have to tell him I have no clue how to get access or I don't have the data.\",\n",
       " '\\nDonny   48:34\\nE it is, yeah.\\nUmm.',\n",
       " '\\nJeroen   48:49\\nSo my other question would be how can I also get the data for the Netherlands now and and Belgium in the future as well?',\n",
       " '\\nDonny   48:56\\nOhh.',\n",
       " \"\\nJeroen   48:57\\nBecause I would love to also get that data and be able to report on it and maybe even be able to do that drill down that you guys can't do because of the the GDPR type issues that.\",\n",
       " '\\nDonny   49:00\\nYeah.\\nYeah.',\n",
       " \"\\nJeroen   49:09\\nSo I'd love to to to spend some time on understanding what you guys have built for the US and how we can also translate that to the Netherlands.\",\n",
       " '\\nDonny   49:16\\nYeah, absolutely.',\n",
       " \"\\nAimee   49:18\\nYeah.\\nAnd we may be able to make that easier for you as we go forward, because we're talking about standing up a data warehouse for Cecile that would contain all of that data, Jerome.\",\n",
       " '\\nJeroen   49:27\\nUmm yeah. Perfect.',\n",
       " \"\\nAimee   49:29\\nAnd then you could instead of going through like some of the hoops we go through to pull it over, you could pull it directly.\\nSo we can definitely share what we've got today and then we can share with you as we get a better solution in place, how you can get that.\",\n",
       " '\\nDonny   49:38\\nYeah.',\n",
       " \"\\nJeroen   49:40\\nYeah.\\nAnd if if you're working with Cecile, then I'm also happy to to wait for a bit to to dial into that one.\",\n",
       " '\\nAimee   49:45\\nIt.',\n",
       " \"\\nJeroen   49:47\\nThere's no immediate question at the moment, since I think the the existing dashboards provide the relevant information that we need, but I would love to be able to do more with it in future so.\",\n",
       " '\\nDonny   49:48\\nYeah.',\n",
       " '\\nAimee   49:58\\nYeah.',\n",
       " \"\\nDonny   50:00\\nYeah, I'd love to.\",\n",
       " \"\\nAimee   50:00\\nNo, I think there's a lot you can do, but I just think there's a better way we can give it to you, a futuristic.\",\n",
       " '\\nJeroen   50:06\\nYeah.',\n",
       " '\\nAimee   50:06\\nSo I wanted to be totally transparent about that.',\n",
       " '\\nJeroen   50:06\\nObviously, no.',\n",
       " \"\\nAimee   50:08\\nSorry, Donny, I didn't read over stuff you, Sir.\",\n",
       " '\\nDonny   50:09\\nYeah.',\n",
       " \"\\nJeroen   50:09\\nThat's fine.\\nThat's fine at happy to wait for for that.\",\n",
       " '\\nDonny   50:11\\nNo, I I yeah.',\n",
       " '\\nJeroen   50:13\\nThat to be placed and no need to start circumventing some of the existing and initiatives.',\n",
       " '\\nDonny   50:18\\nSure.',\n",
       " '\\nAimee   50:20\\nYeah.',\n",
       " \"\\nDonny   50:21\\nI'd.\\nI'd love to get it Dutch perspective on that, that data that's that's respective is always interesting anyway, but I think it'll be fun to see it on the chat data and see what insights you guys can pull off of that.\",\n",
       " '\\nAimee   50:23\\nMm-hmm.',\n",
       " '\\nJeroen   50:31\\nSure, sure.',\n",
       " \"\\nEd   50:32\\nThings like.\\nCan you just let me into the dashboards you're using already?\\nYou know and and and if you could just diverse forward just because eventually there's assist select triples down to a line and it ends up probably my desk as well.\\nSo probably looking at risk due to that and stuff.\",\n",
       " '\\nAimee   50:46\\nYeah, absolutely.\\nHey, Donny, do you mind tossing that out when you send out our presentation?',\n",
       " '\\nDonny   50:47\\nYeah.',\n",
       " \"\\nAimee   50:51\\nAnd then I send out a monthly email too and I'm happy to add you both to that so that you can see what gets sent to them at the same time they get it.\\nIt's updated all the time, but for whatever reason I have to send out a monthly email.\\nSo yes, I will adjust to the August 1 and so that way you have it in your in box too.\",\n",
       " '\\nEd   51:03\\nI know those things.',\n",
       " '\\nJeroen   51:10\\nYeah.',\n",
       " '\\nEd   51:10\\nRising.',\n",
       " '\\nJeroen   51:11\\nThank you.',\n",
       " '\\nAlisdair   51:11\\nDid I ask do you take screenshots of like the perfect and put it in an email and send it out?',\n",
       " '\\nEd   51:11\\nThank you so much.',\n",
       " '\\nAimee   51:13\\nI do.\\nI do, and it pains me.',\n",
       " \"\\nDonny   51:16\\nOne of my one of my favorite conversations that we've ever had in operations where we went back and forth for a long time, why can't we get users to use self service?\",\n",
       " \"\\nAimee   51:18\\nIt's me.\",\n",
       " \"\\nDonny   51:24\\nWhy aren't they doing this?\\nAnd then seconds later, that executive, who can you just email me those reports?\\nI really hate going out there and run them for myself.\\nI'm like, yeah, why don't people use self service?\",\n",
       " '\\nAimee   51:31\\nWhat?',\n",
       " \"\\nDonny   51:33\\nIt's so weird.\",\n",
       " \"\\nEd   51:33\\nIt's a mystery.\",\n",
       " '\\nDonny   51:34\\nUmm.\\nYeah.\\nSo, but there you go.',\n",
       " '\\nJeroen   51:36\\nYeah, you what?\\nYou could create a chat box for that so that they can actually.',\n",
       " '\\nDonny   51:40\\nThere you go.',\n",
       " '\\nAimee   51:40\\nThere you go.',\n",
       " '\\nDonny   51:41\\nThere you go.',\n",
       " '\\nAimee   51:41\\nThere you go.\\nThere you go.\\nTheir next job.\\nPlease go do mine.',\n",
       " \"\\nDonny   51:44\\nSo yeah, but no, I'll I've got some notes here and the dashboard talking about the Ava global stats, right?\",\n",
       " '\\nAimee   51:51\\nThe usage, yeah.',\n",
       " \"\\nDonny   51:51\\nThat's what we're OK.\\nYeah, I'll send that out.\",\n",
       " '\\nJeroen   51:53\\nYeah.',\n",
       " \"\\nDonny   51:53\\nAnd then if you guys don't have access, I may have to import your user.\\nJust email me back and I'll I'll make sure you get a user out there.\\nWe've got a lot so balancing that out for sure, alright.\",\n",
       " '\\nJeroen   52:02\\nPerfect.\\nThank you.',\n",
       " '\\nAlisdair   52:05\\nOK.\\nAndrea was gonna little questions.\\nOK.\\nDonny, honey, thank you very much.',\n",
       " \"\\nEd   52:12\\nIt's a quick no, no, I don't have a question.\",\n",
       " \"\\nAlisdair   52:14\\nSorry, I'd you got one.\",\n",
       " \"\\nEd   52:16\\nJust gonna say it in another really useful session and it's just the commonality of themes about film challenges were facing and but some really great stuff.\",\n",
       " '\\nDonny   52:23\\nYeah.',\n",
       " '\\nEd   52:27\\nYou you clearly work from one of the, so if you see someone just farted person in sunrise as well.',\n",
       " \"\\nDonny   52:33\\nThank you.\\nYeah.\\nNo, it's amazing how how much similarity like the data is different, but we're all we're all facing the same challenges.\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table = pd.DataFrame({\n",
    "    \"Argomento\": [\n",
    "        \"Fonti di dati\", \n",
    "        \"Sicurezza e privacy dei dati\", \n",
    "        \"Strumenti di analisi dei dati\",\n",
    "        \"Collaborazione e allineamento\",\n",
    "        \"Piani e progetti futuri\",\n",
    "        \"Sfide\",\n",
    "        \"Visualizzazioni di dati e dashboard\",\n",
    "        \"Coveo\",\n",
    "        \"Architettura dei dati\",\n",
    "        \"Privacy e sicurezza dei dati\",\n",
    "        \"Amazon Intelligent Document Processing (IDP)\",\n",
    "    ],\n",
    "    \"Dettagli\": [\n",
    "        \"Sono presenti diverse fonti di dati utilizzate per l'analisi, tra cui database SQL, API come Service Cloud e Sales Cloud di Salesforce, e file di esportazione da altri team come Twilio e SAP.\",\n",
    "        \"Si sottolinea l'importanza di proteggere i dati sensibili, in particolare le informazioni personali identificabili (PII), durante l'uso dei dati per ottenere intuizioni.\",\n",
    "        \"Si utilizzano strumenti di analisi dei dati basati su AI e machine learning, come AWS Comprehend e l'Intelligent Document Processing (IDP) di AWS. L'IA e il machine learning rappresentano un'area di rapido sviluppo nell'analisi dei dati.\",\n",
    "        \"Si enfatizza l'importanza della collaborazione e dell'allineamento tra i team. L'incoraggiamento degli utenti a utilizzare gli strumenti di analisi dei dati, la condivisione delle intuizioni attraverso report e dashboard, e l'allineamento su strumenti e metodologie sono elementi chiave.\",\n",
    "        \"Si prevedono piani e progetti futuri per migliorare ulteriormente l'analisi dei dati. Questi includono l'implementazione di nuovi strumenti, l'ampliamento delle fonti di dati e l'ottimizzazione dei processi di analisi dei dati.\",\n",
    "        \"Si affrontano sfide nell'analisi dei dati, tra cui la formazione degli utenti, la gestione dei dati sensibili e l'adattamento ai rapidi cambiamenti nella tecnologia e nelle best practice di analisi dei dati.\",\n",
    "        \"Si sottolinea l'importanza delle visualizzazioni di dati e delle dashboard per l'analisi dei dati e la condivisione delle informazioni. Le dashboard possono aiutare a presentare i dati in un formato visivamente attraente e facilmente comprensibile.\",\n",
    "        \"Si utilizza Coveo nella loro organizzazione. Nonostante Coveo offra alcune funzionalità utili, la sua funzionalità di 'deflessione dell'ultima possibilità' non è stata trovata utile e altre soluzioni hanno fornito risultati superiori.\",\n",
    "        \"Si discute l'architettura dei dati e come vengono gestiti, processati e utilizzati i dati da diverse fonti. Vi è un forte impegno per l'automatizzazione, la standardizzazione e l'utilizzo efficace dei dati.\",\n",
    "        \"Si discutono vari aspetti della privacy e della sicurezza dei dati. Si sottolinea l'importanza della protezione dei dati e della conformità con le leggi sulla privacy dei dati.\",\n",
    "        \"Si utilizza Amazon Intelligent Document Processing (IDP) per analizzare vari tipi di file allegati e redigere i dati sensibili. Amazon IDP è un elemento chiave nella strategia di protezione dei dati.\",\n",
    "    ],\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table.to_excel(r\"C:\\Users\\Raffaele.Sportiello\\OneDrive - Wolters Kluwer\\Documents\\Presentazioni\\TAAE Data & Analytics\\TAAE Data & Analytics Show & Tell NA - Summary.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
