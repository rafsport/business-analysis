{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6hg3-7BCj7M"
      },
      "source": [
        "# Generative AI for News Media\n",
        "\n",
        "The purpose of this notebook is to provide an overview and some examples of how generative models such as GPT-3 can be used for productive purposes in the domain of journalism. It's important to get a handle on the capabilities and limitations of these models in order to use them responsibly. \n",
        "\n",
        "You can try out OpenAI's interactive \"playground\" (https://beta.openai.com/playground) and try variations of the tasks I walk through below. Before we get to tasks though, I'll talk about prompts and model limitations. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGJibeJhhiBK"
      },
      "source": [
        "## Prompting\n",
        "The main way you interact with these kinds of models is through *prompting* -- writing commands in natural language. **The prompt is the textual input to the model used to describe the task that the model should perform.** For example, you could prompt the model with \"Brainstorm three article ideas on the topic of generative AI in news media\" and it should generate a list of three ideas. \n",
        "\n",
        "Prompting is your main way of controlling the machine and providing context or data for the task. You need to communicate clearly and precisely what you want, and what you don't. When writing prompts think about the verbs and objects to describe what you want to happen, like \"<u>Summarize</u> this scientific <u>abstract</u> using lay terminology: \\<abstract text\\>\". \n",
        "\n",
        "Prompting is sometimes referred to as \"prompt design\" or \"prompt engineering\" since you often need to carefully craft and iterate or \"debug\" your prompts so that they accomplish what you want. This is similar to how we explain things to other people: if someone doesn't understand you, you try again and explain it a bit differently, trying to clarify. \n",
        "\n",
        "Here are a few tips for prompting: \n",
        "\n",
        "-- Use plain language, be concrete with verbs and objects\n",
        "\n",
        "-- Iterate on different prompts in the playground so you can quickly see feedback on whether it's working as expected. You might need to try different ways of expressing what you want the model to do.\n",
        "\n",
        "-- Start by being as descriptive as possible and then work backwards to remove extra words and see if performance stays consistent\n",
        "\n",
        "-- Play with the scope and complexity of task: “Write an article about GPT-3” vs. “List three specific advantages of GPT-3 for news media”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aj05wEQhlSm"
      },
      "source": [
        "## Model Limitations\n",
        "Journalists need to be aware of the limitations of the AI systems they use. Generate AI models like GPT-3 generally come with a few caveats including:\n",
        "\n",
        "-- They don't \"reason\" well, especially for complex tasks with multiple steps that require planning. \n",
        "\n",
        "-- They can \"hallucinate\" information leading to factual inaccuracies. They generally cannot provide accurate references or links back to where information came from. \n",
        "\n",
        "-- They can exhibit societal biases based on the data they're trained on. Based on when the training data ends the model won't \"know\" anything about the world after that date (for GPT-3.5 this is June, 2021) \n",
        "\n",
        "-- They don't generally do well on math problems.\n",
        "\n",
        "-- There are length limitations (~3000 words for GPT-3) and generally the longer the output text, the less coherent.\n",
        "\n",
        "-- They may memorize text from their training that could result in copyright issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nByxKMdPFZbR"
      },
      "source": [
        "# Capabilities\n",
        "\n",
        "Next I'll walk through some basic use-cases for generative AI in news media, focusing on text output using the GPT-3 model. I'll demonstrate six basic tasks that should have some journalistic value: (1) Rewriting, (2) Summarization, (3) Brainstorming, (4) Classification, (5) Extraction, and (6) Data-to-text. There are other example tasks you can check out in the OpenAI documentation: https://beta.openai.com/examples as well as in the OpenAI Cookbook: https://github.com/openai/openai-cookbook.  \n",
        "\n",
        "**To edit and run the code, create a copy of this Google Doc in your drive, then sign up on OpenAI for an API key and paste it in below where it says `openai.api_key`.** Or you can also copy-paste the prompts into the OpenAI playground to see the output there.\n",
        "\n",
        "If you're keen to build on these tasks or develop your own, see the Generative AI in the Newsroom Challenge: https://medium.com/@ndiakopoulos/the-generative-ai-in-the-newsroom-challenge-9fe2dc5fb2a7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Tw-jfXTlLxPj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.2-py3-none-any.whl (70 kB)\n",
            "Requirement already satisfied: tqdm in c:\\users\\raffaele.sportiello\\anaconda3\\lib\\site-packages (from openai) (4.64.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\raffaele.sportiello\\anaconda3\\lib\\site-packages (from openai) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.20 in c:\\users\\raffaele.sportiello\\anaconda3\\lib\\site-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\raffaele.sportiello\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\raffaele.sportiello\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (1.26.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\raffaele.sportiello\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\raffaele.sportiello\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\raffaele.sportiello\\anaconda3\\lib\\site-packages (from aiohttp->openai) (5.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\raffaele.sportiello\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\raffaele.sportiello\\anaconda3\\lib\\site-packages (from aiohttp->openai) (4.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\raffaele.sportiello\\anaconda3\\lib\\site-packages (from aiohttp->openai) (21.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\raffaele.sportiello\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\raffaele.sportiello\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\raffaele.sportiello\\anaconda3\\lib\\site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp->openai) (4.4.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\raffaele.sportiello\\anaconda3\\lib\\site-packages (from tqdm->openai) (0.4.4)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.2\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "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"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "MuOp8j_nKRC6",
        "outputId": "76dac68f-3abe-471a-c397-7c2d5ac0c2ba"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "import os\n",
        "\n",
        "# Copy your Open AI Secret Key here between the quotation marks\n",
        "#openai.api_key = os.environ.get('OpenAI_secKeyAPI')\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2L8QVd1g-_K"
      },
      "source": [
        "## 1. Rewriting\n",
        "\n",
        "These models tend to be good at paraphrasing text, which can be useful for removing jargon or simplifying the language of a text. \n",
        "\n",
        "**Use Case**: You're a science reporter trying to see if a recently published paper deals with a topic of interest. So you prompt the model to dejargonize a scientific abstract from a pre-print server. \n",
        "\n",
        "⚠ **WARNING** ⚠ : Output like this should only be used to get a quick sense for what the research is about and help you decide if you want to read the full study.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "zI4zN4RMg8yq",
        "outputId": "b47b5863-6684-41f1-e196-8b086a1778b1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "APIConnectionError",
          "evalue": "Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/completions (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1129)')))",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[0;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1040\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m   1042\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py:414\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    412\u001b[0m     context\u001b[39m.\u001b[39mload_default_certs()\n\u001b[1;32m--> 414\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[0;32m    415\u001b[0m     sock\u001b[39m=\u001b[39;49mconn,\n\u001b[0;32m    416\u001b[0m     keyfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_file,\n\u001b[0;32m    417\u001b[0m     certfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_file,\n\u001b[0;32m    418\u001b[0m     key_password\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_password,\n\u001b[0;32m    419\u001b[0m     ca_certs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_certs,\n\u001b[0;32m    420\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_dir,\n\u001b[0;32m    421\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_data,\n\u001b[0;32m    422\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[0;32m    423\u001b[0m     ssl_context\u001b[39m=\u001b[39;49mcontext,\n\u001b[0;32m    424\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[0;32m    425\u001b[0m )\n\u001b[0;32m    427\u001b[0m \u001b[39m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[39m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \u001b[39m# for the host.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\urllib3\\util\\ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[39mif\u001b[39;00m send_sni:\n\u001b[1;32m--> 449\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(\n\u001b[0;32m    450\u001b[0m         sock, context, tls_in_tls, server_hostname\u001b[39m=\u001b[39;49mserver_hostname\n\u001b[0;32m    451\u001b[0m     )\n\u001b[0;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\urllib3\\util\\ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[39mif\u001b[39;00m server_hostname:\n\u001b[1;32m--> 493\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(sock, server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n\u001b[0;32m    494\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\ssl.py:500\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    496\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    497\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    498\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    499\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[0;32m    501\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[0;32m    502\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[0;32m    503\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[0;32m    504\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[0;32m    505\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[0;32m    506\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    507\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[0;32m    508\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\ssl.py:1040\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1040\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[0;32m   1041\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\ssl.py:1309\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 1309\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[0;32m   1310\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
            "\u001b[1;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1129)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\requests\\adapters.py:440\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 440\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    441\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    442\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    443\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    444\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    445\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    446\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    447\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    448\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    449\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    450\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[0;32m    451\u001b[0m     )\n\u001b[0;32m    453\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:813\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    810\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    811\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[0;32m    812\u001b[0m     )\n\u001b[1;32m--> 813\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murlopen(\n\u001b[0;32m    814\u001b[0m         method,\n\u001b[0;32m    815\u001b[0m         url,\n\u001b[0;32m    816\u001b[0m         body,\n\u001b[0;32m    817\u001b[0m         headers,\n\u001b[0;32m    818\u001b[0m         retries,\n\u001b[0;32m    819\u001b[0m         redirect,\n\u001b[0;32m    820\u001b[0m         assert_same_host,\n\u001b[0;32m    821\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[0;32m    822\u001b[0m         pool_timeout\u001b[39m=\u001b[39mpool_timeout,\n\u001b[0;32m    823\u001b[0m         release_conn\u001b[39m=\u001b[39mrelease_conn,\n\u001b[0;32m    824\u001b[0m         chunked\u001b[39m=\u001b[39mchunked,\n\u001b[0;32m    825\u001b[0m         body_pos\u001b[39m=\u001b[39mbody_pos,\n\u001b[0;32m    826\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mresponse_kw\n\u001b[0;32m    827\u001b[0m     )\n\u001b[0;32m    829\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:813\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    810\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    811\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[0;32m    812\u001b[0m     )\n\u001b[1;32m--> 813\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murlopen(\n\u001b[0;32m    814\u001b[0m         method,\n\u001b[0;32m    815\u001b[0m         url,\n\u001b[0;32m    816\u001b[0m         body,\n\u001b[0;32m    817\u001b[0m         headers,\n\u001b[0;32m    818\u001b[0m         retries,\n\u001b[0;32m    819\u001b[0m         redirect,\n\u001b[0;32m    820\u001b[0m         assert_same_host,\n\u001b[0;32m    821\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[0;32m    822\u001b[0m         pool_timeout\u001b[39m=\u001b[39mpool_timeout,\n\u001b[0;32m    823\u001b[0m         release_conn\u001b[39m=\u001b[39mrelease_conn,\n\u001b[0;32m    824\u001b[0m         chunked\u001b[39m=\u001b[39mchunked,\n\u001b[0;32m    825\u001b[0m         body_pos\u001b[39m=\u001b[39mbody_pos,\n\u001b[0;32m    826\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mresponse_kw\n\u001b[0;32m    827\u001b[0m     )\n\u001b[0;32m    829\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:785\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    783\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[1;32m--> 785\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[0;32m    786\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[0;32m    787\u001b[0m )\n\u001b[0;32m    788\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
            "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/completions (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1129)')))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\openai\\api_requestor.py:516\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[1;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 516\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    517\u001b[0m         method,\n\u001b[0;32m    518\u001b[0m         abs_url,\n\u001b[0;32m    519\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    520\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m    521\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    522\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    523\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[0;32m    525\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\requests\\sessions.py:529\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    528\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 529\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    531\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\requests\\sessions.py:645\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 645\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    647\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\requests\\adapters.py:517\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    516\u001b[0m     \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m    519\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n",
            "\u001b[1;31mSSLError\u001b[0m: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/completions (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1129)')))",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Scripts\\Jupyter script\\Generative_AI_for_News_Media.ipynb Cella 9\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Here is a jargony scientific abstract taken from bioarxiv: https://www.biorxiv.org/content/10.1101/581280v3 \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m abstract \u001b[39m=\u001b[39m \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mCurrent treatments for depression are limited by suboptimal efficacy, delayed response, \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mand frequent side effects. Intermittent theta-burst stimulation (iTBS) is a non-invasive brain stimulation treatment that is \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mresting-state functional connectivity MRI (fcMRI)-guided iTBS protocol for TRD termed \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m‘Stanford Accelerated Intelligent Neuromodulation Therapy (SAINT)’.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m   model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext-davinci-003\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m   prompt\u001b[39m=\u001b[39;49m prompt \u001b[39m+\u001b[39;49m abstract, \u001b[39m# Concatenate the prompt and abstract\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m   temperature\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m, \u001b[39m# Set the temperature to zero to remove variation in the output (should always get the same result)\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m   max_tokens\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m   top_p\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m   frequency_penalty\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   presence_penalty\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X11sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Output the response\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X11sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m (response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m])\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\openai\\api_requestor.py:216\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    207\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m--> 216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[0;32m    217\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[0;32m    218\u001b[0m         url,\n\u001b[0;32m    219\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    220\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    221\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    222\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    223\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    225\u001b[0m     )\n\u001b[0;32m    226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[0;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\openai\\api_requestor.py:528\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[1;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mRequestException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 528\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mAPIConnectionError(\n\u001b[0;32m    529\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError communicating with OpenAI: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)\n\u001b[0;32m    530\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    531\u001b[0m util\u001b[39m.\u001b[39mlog_debug(\n\u001b[0;32m    532\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mOpenAI API response\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    533\u001b[0m     path\u001b[39m=\u001b[39mabs_url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    536\u001b[0m     request_id\u001b[39m=\u001b[39mresult\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mX-Request-Id\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    537\u001b[0m )\n\u001b[0;32m    538\u001b[0m \u001b[39m# Don't read the whole stream for debug logging unless necessary.\u001b[39;00m\n",
            "\u001b[1;31mAPIConnectionError\u001b[0m: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/completions (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1129)')))"
          ]
        }
      ],
      "source": [
        "# Here is the prompt command\n",
        "prompt = \"Rewrite the following abstract to avoid heavy scientific jargon and use simpler vocabulary: \"\n",
        "\n",
        "# Here is a jargony scientific abstract taken from bioarxiv: https://www.biorxiv.org/content/10.1101/581280v3 \n",
        "abstract = \\\n",
        "\"Current treatments for depression are limited by suboptimal efficacy, delayed response, \\\n",
        "and frequent side effects. Intermittent theta-burst stimulation (iTBS) is a non-invasive brain stimulation treatment that is \\\n",
        "FDA-approved for treatment-resistant depression (TRD). Recent methodological advancements suggest iTBS could be improved \\\n",
        "through 1) treating with multiple sessions per day at optimally-spaced intervals, 2) applying a higher overall pulse-dose \\\n",
        "of stimulation and 3) precision targeting of the left dorsolateral prefrontal cortex (L-DLPFC) to subgenual anterior cingulate \\\n",
        "cortex (sgACC) circuit. We examined the feasibility, tolerability, and preliminary efficacy of an accelerated, high-dose, \\\n",
        "resting-state functional connectivity MRI (fcMRI)-guided iTBS protocol for TRD termed \\\n",
        "‘Stanford Accelerated Intelligent Neuromodulation Therapy (SAINT)’.\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt= prompt + abstract, # Concatenate the prompt and abstract\n",
        "  temperature=0.0, # Set the temperature to zero to remove variation in the output (should always get the same result)\n",
        "  max_tokens=300,\n",
        "  top_p=1.0,\n",
        "  frequency_penalty=0.0,\n",
        "  presence_penalty=0.0\n",
        ")\n",
        "\n",
        "# Output the response\n",
        "print (response[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlXaqpe4Cr5Z"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSzaSwBF8f_8"
      },
      "source": [
        "## 2. Summarization\n",
        "\n",
        "These models are quite proficient at crunching a text down so you can scan it more quickly. \n",
        "\n",
        "**Use Case**: You're an AI reporter and want to keep tabs on state legislatures that may be introducing relevant bills. You want to set up an alert so that any bill mentioning \"AI\" gets summarized and sent to your email. \n",
        "\n",
        "⚠ **WARNING** ⚠ : Output like this should only be used to get a quick sense for what the bill is about and help you decide if you want to read the full bill in detail.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "Kq4-7Evf8w5u",
        "outputId": "41e5cca6-a1cb-447d-b393-ffe46b459251"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "APIConnectionError",
          "evalue": "Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/completions (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1129)')))",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[0;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1040\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m   1042\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py:414\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    412\u001b[0m     context\u001b[39m.\u001b[39mload_default_certs()\n\u001b[1;32m--> 414\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[0;32m    415\u001b[0m     sock\u001b[39m=\u001b[39;49mconn,\n\u001b[0;32m    416\u001b[0m     keyfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_file,\n\u001b[0;32m    417\u001b[0m     certfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_file,\n\u001b[0;32m    418\u001b[0m     key_password\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_password,\n\u001b[0;32m    419\u001b[0m     ca_certs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_certs,\n\u001b[0;32m    420\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_dir,\n\u001b[0;32m    421\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_data,\n\u001b[0;32m    422\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[0;32m    423\u001b[0m     ssl_context\u001b[39m=\u001b[39;49mcontext,\n\u001b[0;32m    424\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[0;32m    425\u001b[0m )\n\u001b[0;32m    427\u001b[0m \u001b[39m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[39m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \u001b[39m# for the host.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\urllib3\\util\\ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[39mif\u001b[39;00m send_sni:\n\u001b[1;32m--> 449\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(\n\u001b[0;32m    450\u001b[0m         sock, context, tls_in_tls, server_hostname\u001b[39m=\u001b[39;49mserver_hostname\n\u001b[0;32m    451\u001b[0m     )\n\u001b[0;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\urllib3\\util\\ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[39mif\u001b[39;00m server_hostname:\n\u001b[1;32m--> 493\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(sock, server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n\u001b[0;32m    494\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\ssl.py:500\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    496\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    497\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    498\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    499\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[0;32m    501\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[0;32m    502\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[0;32m    503\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[0;32m    504\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[0;32m    505\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[0;32m    506\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    507\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[0;32m    508\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\ssl.py:1040\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1040\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[0;32m   1041\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\ssl.py:1309\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 1309\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[0;32m   1310\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
            "\u001b[1;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1129)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\requests\\adapters.py:440\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 440\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    441\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    442\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    443\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    444\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    445\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    446\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    447\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    448\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    449\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    450\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[0;32m    451\u001b[0m     )\n\u001b[0;32m    453\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:813\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    810\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    811\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[0;32m    812\u001b[0m     )\n\u001b[1;32m--> 813\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murlopen(\n\u001b[0;32m    814\u001b[0m         method,\n\u001b[0;32m    815\u001b[0m         url,\n\u001b[0;32m    816\u001b[0m         body,\n\u001b[0;32m    817\u001b[0m         headers,\n\u001b[0;32m    818\u001b[0m         retries,\n\u001b[0;32m    819\u001b[0m         redirect,\n\u001b[0;32m    820\u001b[0m         assert_same_host,\n\u001b[0;32m    821\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[0;32m    822\u001b[0m         pool_timeout\u001b[39m=\u001b[39mpool_timeout,\n\u001b[0;32m    823\u001b[0m         release_conn\u001b[39m=\u001b[39mrelease_conn,\n\u001b[0;32m    824\u001b[0m         chunked\u001b[39m=\u001b[39mchunked,\n\u001b[0;32m    825\u001b[0m         body_pos\u001b[39m=\u001b[39mbody_pos,\n\u001b[0;32m    826\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mresponse_kw\n\u001b[0;32m    827\u001b[0m     )\n\u001b[0;32m    829\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:813\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    810\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    811\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[0;32m    812\u001b[0m     )\n\u001b[1;32m--> 813\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murlopen(\n\u001b[0;32m    814\u001b[0m         method,\n\u001b[0;32m    815\u001b[0m         url,\n\u001b[0;32m    816\u001b[0m         body,\n\u001b[0;32m    817\u001b[0m         headers,\n\u001b[0;32m    818\u001b[0m         retries,\n\u001b[0;32m    819\u001b[0m         redirect,\n\u001b[0;32m    820\u001b[0m         assert_same_host,\n\u001b[0;32m    821\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[0;32m    822\u001b[0m         pool_timeout\u001b[39m=\u001b[39mpool_timeout,\n\u001b[0;32m    823\u001b[0m         release_conn\u001b[39m=\u001b[39mrelease_conn,\n\u001b[0;32m    824\u001b[0m         chunked\u001b[39m=\u001b[39mchunked,\n\u001b[0;32m    825\u001b[0m         body_pos\u001b[39m=\u001b[39mbody_pos,\n\u001b[0;32m    826\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mresponse_kw\n\u001b[0;32m    827\u001b[0m     )\n\u001b[0;32m    829\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:785\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    783\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[1;32m--> 785\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[0;32m    786\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[0;32m    787\u001b[0m )\n\u001b[0;32m    788\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
            "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/completions (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1129)')))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\openai\\api_requestor.py:516\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[1;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 516\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    517\u001b[0m         method,\n\u001b[0;32m    518\u001b[0m         abs_url,\n\u001b[0;32m    519\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    520\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m    521\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    522\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    523\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[0;32m    525\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\requests\\sessions.py:529\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    528\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 529\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    531\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\requests\\sessions.py:645\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 645\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    647\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\requests\\adapters.py:517\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    516\u001b[0m     \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m    519\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n",
            "\u001b[1;31mSSLError\u001b[0m: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/completions (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1129)')))",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Scripts\\Jupyter script\\Generative_AI_for_News_Media.ipynb Cella 11\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m bill_text \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mget_text()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Uncomment this if you want to see the original bill text\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#print (bill_text)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m   model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext-davinci-003\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m   prompt\u001b[39m=\u001b[39;49m prompt \u001b[39m+\u001b[39;49m bill_text, \u001b[39m# Concatenate the prompt and bill text\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m   temperature\u001b[39m=\u001b[39;49m\u001b[39m0.7\u001b[39;49m, \u001b[39m# Setting the temperature to the default which can result in some variability from run to run\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m   max_tokens\u001b[39m=\u001b[39;49m\u001b[39m250\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   top_p\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m   frequency_penalty\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m   presence_penalty\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X13sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Output the response\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raffaele.Sportiello/Scripts/Jupyter%20script/Generative_AI_for_News_Media.ipynb#X13sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m (response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m])\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\openai\\api_requestor.py:216\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    207\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m--> 216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[0;32m    217\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[0;32m    218\u001b[0m         url,\n\u001b[0;32m    219\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    220\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    221\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    222\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    223\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    225\u001b[0m     )\n\u001b[0;32m    226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[0;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
            "File \u001b[1;32mc:\\Users\\Raffaele.Sportiello\\Anaconda3\\lib\\site-packages\\openai\\api_requestor.py:528\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[1;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mRequestException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 528\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mAPIConnectionError(\n\u001b[0;32m    529\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError communicating with OpenAI: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)\n\u001b[0;32m    530\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    531\u001b[0m util\u001b[39m.\u001b[39mlog_debug(\n\u001b[0;32m    532\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mOpenAI API response\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    533\u001b[0m     path\u001b[39m=\u001b[39mabs_url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    536\u001b[0m     request_id\u001b[39m=\u001b[39mresult\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mX-Request-Id\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    537\u001b[0m )\n\u001b[0;32m    538\u001b[0m \u001b[39m# Don't read the whole stream for debug logging unless necessary.\u001b[39;00m\n",
            "\u001b[1;31mAPIConnectionError\u001b[0m: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/completions (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1129)')))"
          ]
        }
      ],
      "source": [
        "# Import packages needed to get and parse the data\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Here is the prompt command\n",
        "prompt = \"Summarize what is journalistically newsworthy about the following proposed bill: \"\n",
        "\n",
        "# Download the text for a proposed Massachusetts bill\n",
        "url = \"http://malegislature.gov/Bills/193/SD1827/Senate/Bill/Text\"\n",
        "response = requests.get(url, verify=False)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "bill_text = soup.get_text()\n",
        "\n",
        "# Uncomment this if you want to see the original bill text\n",
        "#print (bill_text)\n",
        "\n",
        "response = openai.Completion.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt= prompt + bill_text, # Concatenate the prompt and bill text\n",
        "  temperature=0.7, # Setting the temperature to the default which can result in some variability from run to run\n",
        "  max_tokens=250,\n",
        "  top_p=1.0,\n",
        "  frequency_penalty=0.0,\n",
        "  presence_penalty=0.0\n",
        ")\n",
        "\n",
        "# Output the response\n",
        "print (response[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB2SbFj-Kk60"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Bu86ihQK4Mm"
      },
      "source": [
        "## 3. Brainstorming\n",
        "\n",
        "These models can be helpful for generating diverse ideas that can spark your creativity. \n",
        "\n",
        "**Use Case**: A press release just landed in your inbox from the NYC Mayor's office. You haven't covered the topic before and don't have much background so you use the model to jumpstart your thinking about possible story angles you could pursue.  \n",
        "\n",
        "**Note**: The use of the `presence_penality` parameter here to nudge the model to produce distinct ideas.\n",
        "\n",
        "⚠ **WARNING** ⚠ : The more you know about a topic, the less the model will probably surprise you. Don't expect any truly creative thinking from the model, but it might offer the right spark for your own creativity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "qfejQ2d7Kp2f",
        "outputId": "0daf4e60-56df-4895-b1e2-fcf28336d987"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "1. \"Mayor Adams Offers Relief to Nearly 200,000 New Yorkers With Unpaid Water Bills\": This article would explore how Mayor Adams' amnesty program is offering relief to nearly 200,000 customers with overdue water bills by forgiving up to 100 percent of interest when customers pay a portion or all of their outstanding water bills. \n",
            "\n",
            "2. \"New Yorkers Could Save up to $150 Million With Temporary Water Bill Amnesty Program\": This article would focus on the potential savings for New Yorkers of up to $150 million that are available through Mayor Adams' temporary water bill amnesty program. \n",
            "\n",
            "3. \"NYC Water Infrastructure Threatened By $1.2 Billion Owed in Unpaid Water Bills\": This article would explain how the $1.2 billion owed in unpaid water bills poses a threat to NYC's water infrastructure and could lead to additional rate hikes if left unpaid.\n"
          ]
        }
      ],
      "source": [
        "# Import packages needed to get and parse the data\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Here is the prompt command\n",
        "prompt = \"Brainstorm three specific news article ideas by critically assessing the following press release, explaining each idea: \"\n",
        "\n",
        "# Download the text for the NYC Press Release\n",
        "url = \"https://www.nyc.gov/office-of-the-mayor/news/077-23/mayor-adams-helps-new-yorkers-save-up-150-million-overdue-water-bills\"\n",
        "response = requests.get(url, verify=False)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "press_release_text = soup.select_one(\".col-content\").get_text()\n",
        "\n",
        "# Uncomment this if you want to see the original press release text\n",
        "#print (bill_text)\n",
        "\n",
        "response = openai.Completion.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt= prompt + press_release_text, # Concatenate the prompt and bill text\n",
        "  temperature=0.7, # Setting the temperature to the default which can result in some variability from run to run\n",
        "  max_tokens=500,\n",
        "  top_p=1.0,\n",
        "  frequency_penalty=0.0,\n",
        "  presence_penalty=0.8 # Setting the presence penality to a relatively high value should help the model brainstorm distinct ideas\n",
        ")\n",
        "\n",
        "# Output the response\n",
        "print (response[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBzLMfWfTimU"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSa6oZrTTXCZ"
      },
      "source": [
        "## 4. Classification\n",
        "\n",
        "Generative models like GPT3 can also be used for analytic tasks like classifying a text into different categories. \n",
        "\n",
        "**Use Case**: You're an engaged journalist doing a follow up story on recent coverage around the need to replace gas cooktops, such as https://www.washingtonpost.com/climate-solutions/2023/02/04/how-to-use-gas-stove-safely/. You want to run a quick survey with people who have switched to induction stoves and ask them why they switched. To find these people you decide to scan the more than 2,700 comments made on the last story. \n",
        "\n",
        "**Note**: The prompt here is designed to output structured data, which can be used easy in downstream processing, like filtering. \n",
        "\n",
        "⚠ **WARNING** ⚠ : Don't make assumptions about what your classifier might be missing. You might need to devise a small study to assess accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PXua9gdcTs6T",
        "outputId": "0301f87a-38e6-4792-edc7-81049022bfab"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing comment 0\n",
            "Testing comment 1\n",
            "Testing comment 2\n",
            "Testing comment 3\n",
            "Testing comment 4\n",
            "Testing comment 5\n",
            "[\n",
            "  {\n",
            "    \"response\": {\n",
            "      \"label\": \"yes\",\n",
            "      \"explanation\": \"The comment describes a positive experience with an induction cooktop, including its efficiency, air fryer function, and the need to purchase new cookware.\"\n",
            "    },\n",
            "    \"comment\": \"We were considering converting to gas when replacing our stove. We went with induction instead. It\\u2019s amazing. Cooks food/boils water faster. Is 90% efficient vs. gas 30% efficacy. Equipped with an air fryer function. We had to purchase new cook wear that is required because of the magnification properties on the burners. You need pots that are made to adhere so to speak to burners while heating. But it\\u2019s worth it! Love our new pots and get dinner ready faster. Now if it would only clean -up.\"\n",
            "  },\n",
            "  {\n",
            "    \"response\": {\n",
            "      \"label\": \"Yes\",\n",
            "      \"explanation\": \"The comment describes the experience of switching from an electric cooktop to an induction cooktop.\"\n",
            "    },\n",
            "    \"comment\": \"When we first moved into our house (we weren't the first owner) it had a conventional electric cooktop. Conventional electric stoves just plain suck for a variety of reasons. But we were lucky, because the builder of our house included both a gas pipe and a 220 volt outlet in the island where the cooktop sat. We considered replacing the electric cooktop with an induction one but didn't want to spend the time, effort and money to replace the majority of our cookware with pots and pans that were magnetic. So in our case it was easy work to yank the electric cooktop and replace it with a gas one. It took the appliance store guys about 30 minutes to make the switch. A couple of years ago we remodeled our kitchen, and in the big scheme of cost and inconvenience, replacing our cookware was pretty trivial. So we finally got our induction cooktop. It combines the best features of electric (more efficient than gas, no air pollution) with the best features of gas (easy and quick to regulate temperature), and includes advantages neither one has (doesn't heat up the kitchen, cooktop doesn't stay burning hot for long when cooking is done), and it heats things faster than either electric or gas. Without a doubt induction cooking is the way of the future. And I'm sure prices will come down as more home owners who can't or don't want to spend on high-end appliances start to demand them.\"\n",
            "  },\n",
            "  {\n",
            "    \"response\": {\n",
            "      \"label\": \"no\",\n",
            "      \"explanation\": \"This comment does not describe someone's experience of switching to an induction cooktop, as they have not yet made the switch.\"\n",
            "    },\n",
            "    \"comment\": \"We have a gas stove, and like it. Our range hood is vented to the outside. We will be getting an induction range within 2 to 4 years. We are older, and it is a safer option since there will not be an open flame. We took this decision before the \\\"controversy\\\" arose. Gas stoves are better than standard electric in some areas (where gas is cheaper), and work very well. It is also true that the products of combustion are a problem. Houseplants help, opening windows helps, range hoods help. Nothing works perfectly.\"\n",
            "  },\n",
            "  {\n",
            "    \"response\": {\n",
            "      \"label\": \"no\",\n",
            "      \"explanation\": \"The comment does not describe someone's experience of switching to an induction cooktop.\"\n",
            "    },\n",
            "    \"comment\": \"Unless you have a small home with little ventilation, the risk from gas stove emissions is quite small. After cooking on a induction stove top for a year in a rental we opted for gas when we put a new stove in our own house. It is well ventilated and this old house provides a lot of unplanned ventilation too.\"\n",
            "  },\n",
            "  {\n",
            "    \"response\": {\n",
            "      \"label\": \"no\",\n",
            "      \"explanation\": \"This comment does not describe someone's experience of switching to an induction cooktop.\"\n",
            "    },\n",
            "    \"comment\": \"And just so we're all clear: the suitability of a pan for induction cooking is not \\\"adhesion\\\". Testing with a magnet is a quick way to determine whether the pan has enough ferrous metal to respond to the magnetic field generate by the element.\"\n",
            "  },\n",
            "  {\n",
            "    \"response\": {\n",
            "      \"label\": \"no\",\n",
            "      \"explanation\": \"The comment does not describe someone's experience of switching to an induction cooktop.\"\n",
            "    },\n",
            "    \"comment\": \"The gas stove hullabaloo is absurd. The matter of the plastic straws is infinitely more serious because they stay out there essentially forever and affect other creatures. I have no problem banning them.\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import time\n",
        "from openai.error import RateLimitError\n",
        "\n",
        "# For purposes of the demo we just scan 6 comments, but this could easily be scaled up.\n",
        "sample_comments = [\"We were considering converting to gas when replacing our stove. We went with induction instead. It’s amazing. Cooks food/boils water faster. Is 90% efficient vs. gas 30% efficacy. Equipped with an air fryer function. We had to purchase new cook wear that is required because of the magnification properties on the burners. You need pots that are made to adhere so to speak to burners while heating. But it’s worth it! Love our new pots and get dinner ready faster. Now if it would only clean -up.\",\n",
        "     \"When we first moved into our house (we weren't the first owner) it had a conventional electric cooktop. Conventional electric stoves just plain suck for a variety of reasons. But we were lucky, because the builder of our house included both a gas pipe and a 220 volt outlet in the island where the cooktop sat. We considered replacing the electric cooktop with an induction one but didn't want to spend the time, effort and money to replace the majority of our cookware with pots and pans that were magnetic. So in our case it was easy work to yank the electric cooktop and replace it with a gas one. It took the appliance store guys about 30 minutes to make the switch. A couple of years ago we remodeled our kitchen, and in the big scheme of cost and inconvenience, replacing our cookware was pretty trivial. So we finally got our induction cooktop. It combines the best features of electric (more efficient than gas, no air pollution) with the best features of gas (easy and quick to regulate temperature), and includes advantages neither one has (doesn't heat up the kitchen, cooktop doesn't stay burning hot for long when cooking is done), and it heats things faster than either electric or gas. Without a doubt induction cooking is the way of the future. And I'm sure prices will come down as more home owners who can't or don't want to spend on high-end appliances start to demand them.\",\n",
        "     \"We have a gas stove, and like it. Our range hood is vented to the outside. We will be getting an induction range within 2 to 4 years. We are older, and it is a safer option since there will not be an open flame. We took this decision before the \\\"controversy\\\" arose. Gas stoves are better than standard electric in some areas (where gas is cheaper), and work very well. It is also true that the products of combustion are a problem. Houseplants help, opening windows helps, range hoods help. Nothing works perfectly.\",\n",
        "     \"Unless you have a small home with little ventilation, the risk from gas stove emissions is quite small. After cooking on a induction stove top for a year in a rental we opted for gas when we put a new stove in our own house. It is well ventilated and this old house provides a lot of unplanned ventilation too.\",\n",
        "     \"And just so we're all clear: the suitability of a pan for induction cooking is not \\\"adhesion\\\". Testing with a magnet is a quick way to determine whether the pan has enough ferrous metal to respond to the magnetic field generate by the element.\",\n",
        "     \"The gas stove hullabaloo is absurd. The matter of the plastic straws is infinitely more serious because they stay out there essentially forever and affect other creatures. I have no problem banning them.\"\n",
        "    ]\n",
        "\n",
        "# Here is the prompt command\n",
        "prompt = 'Does the following comment describe someone\\'s experience of switching to an induction cooktop? Respond with JSON structured data with a \"label\" field that is either \"yes\" or \"no\" and and \"explanation\" field that provides an explanation of why it\\'s labeled that way: '\n",
        "\n",
        "# This array will store our categorized data\n",
        "output_data = []\n",
        "\n",
        "# Loop through our list of sample_comments\n",
        "for ncomment, comment_text in enumerate(sample_comments):\n",
        "  print (\"Testing comment\", ncomment)\n",
        "\n",
        "  # Construct the API request  \n",
        "  try:\n",
        "    response = openai.Completion.create(\n",
        "      model=\"text-davinci-003\",\n",
        "      prompt= prompt + comment_text, \n",
        "      temperature=0.0, # We don't want any variability in the responses \n",
        "      max_tokens=256,\n",
        "      top_p=1.0,\n",
        "      frequency_penalty=0.0,\n",
        "      presence_penalty=0.0\n",
        "    )\n",
        "\n",
        "    data = {}\n",
        "    data[\"response\"] = json.loads(response[\"choices\"][0][\"text\"])\n",
        "    data[\"comment\"] = comment_text\n",
        "    output_data.append(data)\n",
        "    #print (data)\n",
        "\n",
        "    # You need to sleep for 3 seconds in between API calls to avoid being rate limited (the rate limit is 20 calls / minute for the free tier)\n",
        "    time.sleep(5)\n",
        "  except RateLimitError as e:\n",
        "    print (e)\n",
        "    time.sleep (10) # Wait some more\n",
        "\n",
        "# Outcome the final data\n",
        "print (json.dumps(output_data, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpcL7XgPToNA"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC5QCa-kTXKi"
      },
      "source": [
        "## 5. Extraction\n",
        "\n",
        "Models can also take unstructured data like documents and structure it for further analysis. \n",
        "\n",
        "**Use Case**: Roberto Rocha posted an interesting use case here https://robertorocha.info/getting-tabular-data-from-unstructured-text-with-gpt-3-an-ongoing-experiment/ The idea is to extract structured data about who is lobbying government officials from the Candian federal lobbyist registry. This is quite a messy data source.  \n",
        "\n",
        "⚠ **WARNING** ⚠ : It took a lot of prompt iteration for this one to get it to work as intended even just on the small sample of documents in the demo. Further testing would be required to confidently generalize it to a larger dataset.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "yu1gjvGTTtu7",
        "outputId": "bb4e1a06-9ebf-45d7-a4aa-cc80557eb6e2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "1996 | 1997 | Executive Assistant Minister of Transport\n",
            "\n",
            "\n",
            "1991 | 1993 | Special Assistant Hon. Robert Kaplan\n",
            "\n",
            "\n",
            "September 1984 | February 1988 | Senior Policy Analyst\n",
            "\n",
            "\n",
            "January 2002 | May 2002 | Chief of Staff, Office of the Minister of Public Works and Government Services Canada\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from openai.error import RateLimitError\n",
        "\n",
        "# A sample of 4 documents we might want to parse\n",
        "sample_documents = [\"1996-1997 EXECUTIVE ASSISTANT MINISTER OF TRANSPORT\",\n",
        "\"Special Assistant 1991 to 1993 Hon. Robert Kaplan\",\n",
        "\"September 1984 to February 1988 Senior Policy Analyst - various assignments related to federal procurement and trade policy Department of Supply and Services\",\n",
        "\"January 2002 to May 2002 Chief of Staff Office of the Minister of Public Works and Government Services Canada\"\n",
        "]\n",
        "\n",
        "# Here is the prompt command\n",
        "prompt = 'Extract two dates or years from the input data. Also extract a job description from each line of text in the input data. Create a three-column table with the first date, second date, and job description. If there is no date or job description, leave the column blank. \\\n",
        "Use the following format: first date | second date | job description \\\n",
        "input data: '\n",
        "\n",
        "# Loop through our list of sample_comments\n",
        "for ndoc, document_text in enumerate(sample_documents):\n",
        "  # Construct the API request  \n",
        "  try:\n",
        "    response = openai.Completion.create(\n",
        "      model=\"text-davinci-003\",\n",
        "      prompt= prompt + document_text, \n",
        "      temperature=0.7,  \n",
        "      max_tokens=256,\n",
        "      top_p=1.0,\n",
        "      frequency_penalty=0.0,\n",
        "      presence_penalty=0.0\n",
        "    )\n",
        "\n",
        "    print (response[\"choices\"][0][\"text\"])\n",
        "\n",
        "    # You need to sleep for 3 seconds in between API calls to avoid being rate limited (the rate limit is 20 calls / minute for the free tier)\n",
        "    time.sleep(3)\n",
        "  except RateLimitError as e:\n",
        "    print (e)\n",
        "    time.sleep (10) # Wait some more\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNKkaiXjTqYc"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIuCH1ryTXTw"
      },
      "source": [
        "## 6. Structured Data to Text\n",
        "\n",
        "These models can take structured data and output readable text to support various data journalism projects. \n",
        "\n",
        "**Use Case**: You want to create a quick blurb for your website that summarizes the CDC weekly Influenze Surveillance Data (https://www.cdc.gov/flu/weekly/index.htm) and personalizes it the location (e.g. state) of the user. \n",
        "\n",
        "⚠ **WARNING** ⚠ : When generating text, always be sure to double-check it for factual accuracy before publishing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "zfXnXPSvTfMC",
        "outputId": "23ebcf7e-9c34-4cf6-e12e-99b618ec9d40"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "This week, influenza activity in Virginia has been moderate. Visits to emergency departments or urgent care centers have reported an ILI rate of 3.2%.\n"
          ]
        }
      ],
      "source": [
        "# Here is the prompt command\n",
        "prompt = \"Write a short description of this week's influenza activity based on the following data: \"\n",
        "\n",
        "# And some sample data\n",
        "data = \"State: Virginia \\\n",
        "ILI Activity Level: Moderate \\\n",
        "ILI Rate for visits to emergency departments or urgent care centers: 3.2%\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt= prompt + data, \n",
        "  temperature=0.7, \n",
        "  max_tokens=256,\n",
        "  top_p=1.0,\n",
        "  frequency_penalty=0.0,\n",
        "  presence_penalty=0.0\n",
        ")\n",
        "\n",
        "# Output the response\n",
        "print (response[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFxEeTliu0mQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
